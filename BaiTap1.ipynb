{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HienNguyenMinh/PythonAssignment/blob/main/BaiTap1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Giải Thích Để Hiểu Về M5-Forecasting Bằng Tiếng Việt\n",
        "#Giới thiệu:\n",
        "Trong cuộc thi tên là M5-Forecasting, những người tham gia cuộc thi sử dụng dữ liệu bán hàng phân cấp được trích xuất từ Walmart cho việc phân tích và dự đoán doanh số hàng ngày trong 28 ngày tới. *(Walmart là công ty lớn nhất thế giới theo doanh thu)*.\n",
        "Tập dữ liệu bán hàng chứa thông tin của khoảng 30.000 mặt hàng khác nhau trong gần 1900 ngày. Dữ liệu được phân bổ quanh các cửa hàng ở ba tiêu bang Hoa Kỳ (California, Texa và Wisconsin) bao gồm các cấp mặt hàng, bộ phận, danh mục sản phẩm và cấp chi tiết cửa hàng. Ngoài ra, tập dữ liệu cũng có các biến số giải thích về giá cả, các chương trình khuyến mãi, ngày trong tuần và các sự kiện đặc biệt. Hơn nữa, tập dữ liệu mạnh mẽ này không chỉ giúp cải thiện độ chính xác của dự báo, mà còn giúp hiển thị các mô hình bán hàng cơ bản thông qua các ngôn ngữ khác nhau cũng như dựa trên các danh mục sản phẩm khác nhau cung cấp giá trị cho các tầm nhìn kinh doanh.\n",
        "\n",
        "Theo thực tế là một tập dữ liệu lớn được tạo sẵn, chúng ta bắt đầu với một công thức kinh doanh được đưa ra xoay quanh vài vấn đề. Liệu tập dữ liệu này có đủ để trả lời các câu hỏi đó hay không. Nếu câu trả lời chưa được tìm thấy từ tập dữ liệu này, thì ít nhất các thông tin cần thiết theo tập điểm dữ liệu cũng giúp cải thiện việc thiết kế dữ liệu trong tương lai. \n",
        "\n",
        "#Báo cáo vấn đề\n",
        "Một tập dữ liệu lớn khiến cho việc tìm kiếm tất cả các mẫu cơ bản trong tập dữ liệu đó trở nên khó khăn. May mắn thay, việc đặt ra các hỏi có tính chất SMART rất hữu ích với việc hiểu các mẫu mà có thể vô tình không nhìn thấy nó trong lần đầu tiếp cận vào tập dữ liệu. *(SMART: cụ thể, có thể đo lường, có thể đạt được, có liên quan và có giới hạn thời gian)*.\n",
        "\n",
        "Trong các vấn đề cụ thể của m5-forecasting đặc biệt quan tâm việc tìm ra câu trả lời cho các câu hỏi sau:\n",
        "\n",
        "* Sự phân phối chung của các ID mặt hàng trên các danh mục là gì?\n",
        "* Hành vi của các danh mục trên các cửa hàng khác nhau là gì?\n",
        "* ID của mặt hàng bán chạy nhất?\n",
        "* ID của mặt hàng trong mỗi danh mục có nhiều doanh thu nhất?\n",
        "* Doanh thu bán hàng thu được nhiều nhất là bao nhiêu? Nó có phải là ID của các mặt hàng bán chạy nhất, mang lại doanh thu cao nhất, hoặc có thay đổi theo xu hướng không?\n",
        "* Hành vi của tổng doanh số bán hàng là gì?\n",
        "* Hành vi của ID các mặt hàng đang bán trong các ngày khác nhau trong tuần là gì? Có ngày cụ thể nào mà hôm đó có doanh thu cao nhất? Xu hướng thay đổi như thế nào ở các cửa hàng khác nhau?\n",
        "\n",
        "Và cuối cùng, chúng tôi sẽ sử dụng SARIMAX và Facebook đã phát triển mô hình Prophet để dự báo doanh số bán hàng trong 28 ngày tới.\n",
        "\n",
        "#Các chỉ số đánh giá:\n",
        "Để đưa ra dự báo một chuổi thời gian (time series), chung ta dùng các error metrics sau:\n",
        "1. RMSE (Root Mean Square Error): Để đánh giá dự báo bán hàng được thực hiện bằng SARIMAX, RMSE error metric được sử dụng. Chuỗi thời gian được sử dụng trong trường hợp này, lấy số lượng bán hàng trung bình được thực hiện, trong suốt gần 1900 ngày, bỏ qua việc sắp xếp dữ liệu thứ cấp ban đầu của id sản phẩm cùng với một số cấp độ khác. Mức độ tối đa của error metric RMSE không bị vô hiệu vì chúng tôi đang xem xét số lượng bán hàng trung bình được thực hiện, do đó tận dụng hiệu quả của rất nhiều số 0 đối với nhiều id sản phẩm, bên cạnh số ngày.\n",
        "2. Custom Loss (WMAPE): error metric này được sử dụng cho những trường hợp mà mức độ ưu tiên của ID sản phẩm được xem xét cùng với doanh số đã thực hiện. Các trọng số được tính toán bằng cách chia sự khác biệt giữa dự báo và giá trị bán hàng thực tế cho giá trị trung bình của chúng.\n",
        "3. WRMSSE (Weighted Root Mean Square Scaled Error): WRMSSE được tính khi khung dữ liệu sẵn sàng dự đoán (dự báo gần 30.000 id sản phẩm) đã sẵn sàng. Việc tính toán WRMSSE yêu cầu phải sử dụng tổng số mục nhập của tất cả id sản phẩm (gần 30.000 sản phẩm) và sau đó là sự khác biệt giữa các giá trị của tập dữ liệu xác thực, tức là sales_train_validation.csv và khung dữ liệu được dự báo. WRMSSE eror metirc này là chỉ số do cuộc thi cung cấp và đã được tối ưu hóa để sử dụng cho cuộc thi hiện tại. Để đánh giá nó, chúng tôi sẽ phải sử dụng toàn bộ tập dữ liệu của sales_train_validation.csv đã được cung cấp trong một tệp csv.\n"
      ],
      "metadata": {
        "id": "VM0XZcspKJn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tải các thư viện cần dùng: \n",
        "\n",
        "* Tải các module như numpy, pandas, pyplot, time, math, datetime, widgets..Dùng dòng lệnh: <font color='blue'> import <tên module> </font> hoặc <font color='blue'>import <tên module> as <định danh></font>\n",
        "\n",
        "* Tải các hàm như display, KDTree...Dùng dòng lệnh: <font color='blue'>from <tên module> import <tên hàm></font>"
      ],
      "metadata": {
        "id": "yOkScJOZQo_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Import widgets\n",
        "from ipywidgets import widgets, interactive, interact\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "from math import log, floor\n",
        "from sklearn.neighbors import KDTree\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import pywt\n",
        "from statsmodels.robust import mad\n",
        "\n",
        "import scipy\n",
        "import statsmodels\n",
        "from scipy import signal\n",
        "\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "plt.style.use('seaborn')\n",
        "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n"
      ],
      "metadata": {
        "id": "Ua54I91BDqnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PC5JGxEWUEJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gdown -y && pip install gdown\n",
        "!rmdir dataset\n",
        "!mkdir dataset\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/folders/1xzhS2q9hbUgpftNYf_bYA-wPun8DV9mE?usp=sharing -O /dataset\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/dataset'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "LZBGcWHSJAfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79399cf-57de-495b-807c-078c5ea6f9d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gdown 4.5.3\n",
            "Uninstalling gdown-4.5.3:\n",
            "  Successfully uninstalled gdown-4.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gdown\n",
            "  Using cached gdown-4.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-4.5.3\n",
            "Retrieving folder list\n",
            "Processing file 1Xp7FWyBxoGrdoqAaXjS6M7sURXd5fuO- calendar.csv\n",
            "Processing file 1pIpFsU8X64_wsnP9XFJ4zaoy91YOAeoE sales_train_evaluation.csv\n",
            "Processing file 1YsN6EtxzCsh63MIHGs7ba9eR5UB3DgJt sales_train_validation.csv\n",
            "Processing file 1yzjUhX3Apq0QfdUiGRyjrAFb3uKwm0e2 sample_submission.csv\n",
            "Processing file 1zW7ggFfynmie-NFaLl-EwtUpZ7RXsBqx sell_prices.csv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Xp7FWyBxoGrdoqAaXjS6M7sURXd5fuO-\n",
            "To: /dataset/calendar.csv\n",
            "100% 103k/103k [00:00<00:00, 74.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pIpFsU8X64_wsnP9XFJ4zaoy91YOAeoE\n",
            "To: /dataset/sales_train_evaluation.csv\n",
            "100% 122M/122M [00:00<00:00, 170MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YsN6EtxzCsh63MIHGs7ba9eR5UB3DgJt\n",
            "To: /dataset/sales_train_validation.csv\n",
            "100% 120M/120M [00:00<00:00, 247MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yzjUhX3Apq0QfdUiGRyjrAFb3uKwm0e2\n",
            "To: /dataset/sample_submission.csv\n",
            "100% 5.23M/5.23M [00:00<00:00, 173MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zW7ggFfynmie-NFaLl-EwtUpZ7RXsBqx\n",
            "To: /dataset/sell_prices.csv\n",
            "100% 203M/203M [00:01<00:00, 191MB/s]\n",
            "Download completed\n",
            "/dataset/sales_train_evaluation.csv\n",
            "/dataset/calendar.csv\n",
            "/dataset/sell_prices.csv\n",
            "/dataset/sales_train_validation.csv\n",
            "/dataset/sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mô tả Dataset:\n",
        "\n",
        "Các file data được tải vào notebook này như sau:\n",
        "\n",
        "* calendar_df.csv - chứa cá thông tin về những ngày mà sản phẩm được bán, thông tin về những ngày lễ và các dịp đặc biệt.\n",
        "* train_sales_df.csv - Contains the historical daily unit sales data per product and store and department id with almost a data of sales for 1900 days [d_1 - d_1913]\n",
        "* submission_file.csv - The correct format for submission file, containing the product ids and the column ids for the next 28 days sales data forecast.\n",
        "sell_prices_df.csv - Contains information about the price of the products sold per store and date.\n",
        "* sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)"
      ],
      "metadata": {
        "id": "L2zja3WQDfqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sell_prices_df = pd.read_csv('/dataset/sell_prices.csv')\n",
        "train_sales_df = pd.read_csv('/dataset/sales_train_validation.csv')\n",
        "calendar_df = pd.read_csv('/dataset/calendar.csv')\n",
        "submission_file = pd.read_csv('/dataset/sample_submission.csv')"
      ],
      "metadata": {
        "id": "tj_pZirOTcMV"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}